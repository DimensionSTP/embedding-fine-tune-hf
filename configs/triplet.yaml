# @package _global_
defaults:
  - dataset: triplet_dataset
  - loss: triplet_loss
  - training_arguments: triplet_config
  - trainer: triplet_trainer
  - evaluator: triplet_evaluator
  - deepspeed: ds_config
  - hydra: hydra

package_name: embedding-fine-tune-hf
project_dir: ${oc.env:PROJECT_DIR}/${package_name}
connected_dir: ${oc.env:CONNECTED_DIR}/${package_name}

fine_tune_method: triplet

seed: 2025

data_type: structural

batch_size: 16
eval_batch_size: 16
workers_ratio: 8
use_all_workers: false

split_ratio: 1e-2
is_strict_split: false

dataset_name: triplet
dataset_format: parquet
anchor_column_name: anchor
positive_column_name: positive
negative_column_name: negative
upload_user: Qwen
model_type: Qwen3-Embedding-4B
pretrained_model_name: ${upload_user}/${model_type}
revision: main

user_name: ${oc.env:USER_NAME}
model_detail: ${model_type}
upload_tag: triplet
model_path: ${user_name}/${model_detail}-${upload_tag}

is_peft: false
peft_config:
  r: 64
  lora_alpha: 16
  target_modules: all-linear
  lora_dropout: 0.1
  bias: none
  task_type: CAUSAL_LM
  inference_mode: false

gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

optim: paged_adamw_32bit
lr: 3e-6
weight_decay: 1e-1

scheduler_type: cosine
warmup_ratio: 5e-2

devices: ${oc.decode:${oc.env:DEVICES,null}}
strategy: deepspeed
logging_steps: 10
is_bf16: true
gradient_accumulation_steps: 1
max_grad_norm: 1
epoch: 2
step: 100

model_name: Embedding-Triplet
mode: train

project_name: ${model_name}-${dataset_name}-${mode}
total_batch_size: bs=${batch_size}x${devices}x${gradient_accumulation_steps}
save_detail: ${model_detail}-is_peft=${is_peft}-${total_batch_size}
logging_name: ${save_detail}-lr${lr}

output_dir: ${connected_dir}/checkpoints/${model_name}/${dataset_name}/${strategy}/${save_detail}
save_strategy: steps
save_total_limit: -1

resume_training: false
resume_from_checkpoint: null
use_validation: true
eval_strategy: epoch

run_name: ${project_name}
work_dir: ${hydra:runtime.cwd}
